<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Vivek Ramanujan</title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@300;400;500;600;700&display=swap" rel="stylesheet" />
    <style>
      :root {
        --bg: #0a0a0a;
        --fg: #e8e8e8;
        --accent: #fff;
        --muted: #666;
        --border: #333;
        --highlight: #ff3e00;
      }

      [data-theme="light"] {
        --bg: #fafafa;
        --fg: #1a1a1a;
        --accent: #000;
        --muted: #888;
        --border: #ddd;
        --highlight: #ff3e00;
      }

      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }

      html {
        font-size: 16px;
      }

      body {
        font-family: 'JetBrains Mono', monospace;
        background: var(--bg);
        color: var(--fg);
        line-height: 1.7;
        min-height: 100vh;
        transition: background 0.3s, color 0.3s;
      }

      .container {
        max-width: 900px;
        margin: 0 auto;
        padding: 4rem 2rem;
      }

      @media (max-width: 600px) {
        .container {
          padding: 2rem 1.5rem;
        }
      }

      /* Theme Toggle */
      .theme-toggle {
        position: fixed;
        top: 2rem;
        right: 2rem;
        background: var(--border);
        border: none;
        width: 48px;
        height: 48px;
        cursor: pointer;
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: 1.2rem;
        z-index: 100;
      }

      .theme-toggle:hover {
        background: var(--highlight);
        color: #fff;
      }

      /* Header */
      .header {
        display: grid;
        grid-template-columns: 1fr auto;
        gap: 2rem;
        align-items: start;
        margin-bottom: 4rem;
        padding-bottom: 2rem;
        border-bottom: 4px solid var(--fg);
      }

      @media (max-width: 600px) {
        .header {
          grid-template-columns: 1fr;
          gap: 1.5rem;
        }
      }

      .profile-image {
        width: 140px;
        height: 140px;
        object-fit: cover;
        filter: grayscale(100%);
        transition: filter 0.3s;
      }

      .profile-image:hover {
        filter: grayscale(0%);
      }

      @media (max-width: 600px) {
        .profile-image {
          width: 100px;
          height: 100px;
          order: -1;
        }
      }

      h1 {
        font-size: clamp(2rem, 8vw, 4rem);
        font-weight: 700;
        letter-spacing: -0.03em;
        line-height: 1.1;
        margin-bottom: 1.5rem;
        text-transform: uppercase;
      }

      .bio {
        font-size: 0.95rem;
        color: var(--fg);
        max-width: 600px;
      }

      .bio a {
        color: var(--highlight);
        text-decoration: none;
        border-bottom: 1px solid transparent;
      }

      .bio a:hover {
        border-bottom-color: var(--highlight);
      }

      /* Links Bar */
      .links {
        display: flex;
        flex-wrap: wrap;
        gap: 0.5rem;
        margin-top: 2rem;
      }

      .links a {
        display: inline-block;
        padding: 0.5rem 1rem;
        background: var(--fg);
        color: var(--bg);
        text-decoration: none;
        font-size: 0.8rem;
        font-weight: 500;
        text-transform: uppercase;
        letter-spacing: 0.05em;
        transition: background 0.2s, transform 0.2s;
      }

      .links a:hover {
        background: var(--highlight);
        color: #fff;
        transform: translateY(-2px);
      }

      /* Sections */
      .section {
        margin-bottom: 4rem;
      }

      h2 {
        font-size: 0.8rem;
        font-weight: 600;
        text-transform: uppercase;
        letter-spacing: 0.15em;
        color: var(--muted);
        margin-bottom: 2rem;
        padding-bottom: 0.5rem;
        border-bottom: 1px solid var(--border);
      }

      /* Papers */
      .paper {
        margin-bottom: 3rem;
        padding-left: 1rem;
        border-left: 2px solid var(--border);
        transition: border-color 0.2s;
      }

      .paper:hover {
        border-left-color: var(--highlight);
      }

      .paper-title {
        font-size: 1.1rem;
        font-weight: 600;
        color: var(--fg);
        margin-bottom: 0.5rem;
        text-decoration: none;
        display: block;
      }

      .paper-title:hover {
        color: var(--highlight);
      }

      .paper-authors {
        font-size: 0.85rem;
        color: var(--muted);
        margin-bottom: 0.5rem;
      }

      .paper-authors .me {
        color: var(--fg);
        font-weight: 500;
      }

      .paper-venue {
        font-size: 0.8rem;
        color: var(--highlight);
        text-transform: uppercase;
        letter-spacing: 0.05em;
        margin-bottom: 1rem;
      }

      .paper-description {
        font-size: 0.9rem;
        color: var(--fg);
        opacity: 0.8;
        margin-bottom: 1rem;
      }

      .paper-links {
        display: flex;
        gap: 1rem;
      }

      .paper-links a {
        font-size: 0.75rem;
        color: var(--muted);
        text-decoration: none;
        text-transform: uppercase;
        letter-spacing: 0.05em;
      }

      .paper-links a:hover {
        color: var(--highlight);
      }

      /* Service Grid */
      .service-grid {
        display: grid;
        gap: 0.5rem;
      }

      .service-item {
        display: grid;
        grid-template-columns: 140px 1fr;
        gap: 1rem;
        font-size: 0.9rem;
        padding: 0.5rem 0;
        border-bottom: 1px solid var(--border);
      }

      @media (max-width: 600px) {
        .service-item {
          grid-template-columns: 1fr;
          gap: 0.25rem;
        }
      }

      .service-item .role {
        color: var(--muted);
        text-transform: uppercase;
        font-size: 0.75rem;
        letter-spacing: 0.05em;
      }

      .service-item .event {
        color: var(--fg);
      }

      .service-item a {
        color: var(--highlight);
        text-decoration: none;
      }

      /* Footer */
      .footer {
        margin-top: 4rem;
        padding-top: 2rem;
        border-top: 1px solid var(--border);
        font-size: 0.75rem;
        color: var(--muted);
      }

      .note {
        font-size: 0.8rem;
        color: var(--muted);
        margin-bottom: 2rem;
      }
    </style>
  </head>
  <body>
    <button class="theme-toggle" onclick="toggleTheme()" aria-label="Toggle theme">
      <span id="theme-icon">◐</span>
    </button>

    <div class="container">
      <header class="header">
        <div class="bio-section">
          <h1>Vivek<br>Ramanujan</h1>
          <p class="bio">
            PhD student at the <a href="https://cs.washington.edu">University of Washington</a>,
            working with <a href="https://homes.cs.washington.edu/~ali/">Ali Farhadi</a> and
            <a href="https://people.csail.mit.edu/ludwigs/">Ludwig Schmidt</a> on robust machine learning.
            Previously predoctoral researcher at <a href="https://prior.allenai.org">AI2</a>.
          </p>
          <div class="links">
            <a href="mailto:ramanv@cs.washington.edu">Email</a>
            <a href="cv.pdf">CV</a>
            <a href="https://www.github.com/vkramanuj">Github</a>
            <a href="https://scholar.google.com/citations?user=yXFPyNMAAAAJ&hl=en">Scholar</a>
          </div>
        </div>
        <img src="./images/vivekr.png" alt="Vivek Ramanujan" class="profile-image" />
      </header>

      <section class="section">
        <h2>Research</h2>
        <p class="note">* denotes equal contribution</p>

        <article class="paper">
          <a href="https://arxiv.org/abs/2412.16326" class="paper-title">
            When Worse is Better: Navigating the Compression-Generation Tradeoff in Visual Tokenization
          </a>
          <div class="paper-authors">
            <span class="me">Vivek Ramanujan</span>, Kushal Tirumala, Armen Aghajanyan, Luke Zettlemoyer, Ali Farhadi
          </div>
          <div class="paper-venue">NeurIPS 2025 — Spotlight</div>
          <div class="paper-description">
            Studies when and how better image reconstruction leads to better generation. CRT achieves state-of-the-art ImageNet generation (2.18 FID) with 2-3x improved compute efficiency even with worse reconstruction.
          </div>
        </article>
 even with worse reconstruction
        <article class="paper">
          <a href="https://arxiv.org/abs/2412.07770" class="paper-title">
            From an Image to a Scene: Learning to Imagine the World from a Million 360 Videos
          </a>
          <div class="paper-authors">
            Matthew Wallingford, Anand Bhattad, Aditya Kusupati, <span class="me">Vivek Ramanujan</span>, Matt Deitke, Sham Kakade, Aniruddha Kembhavi, Roozbeh Mottaghi, Wei-Chiu Ma, Ali Farhadi
          </div>
          <div class="paper-venue">NeurIPS 2024</div>
          <div class="paper-description">
            360-1M dataset and Odin model for novel view synthesis from the largest real-world multi-view dataset to date.
          </div>
        </article>

        <article class="paper">
          <a href="https://arxiv.org/abs/2406.05184" class="paper-title">
            The Unmet Promise of Synthetic Training Images: Using Retrieved Real Images Performs Better
          </a>
          <div class="paper-authors">
            Scott Geng, Cheng-Yu Hsieh, <span class="me">Vivek Ramanujan</span>, Matthew Wallingford, Chun-Liang Li, Pang Wei Koh, Ranjay Krishna
          </div>
          <div class="paper-venue">NeurIPS 2024</div>
          <div class="paper-description">
            Synthetic data can be beneficial, but is consistently matched or outperformed by real images from a simple retrieval baseline.
          </div>
          <div class="paper-links">
            <a href="https://github.com/scottgeng00/unmet-promise">Code</a>
          </div>
        </article>

        <article class="paper">
          <a href="https://arxiv.org/abs/2307.12532" class="paper-title">
            On the Connection between Pre-training Data Diversity and Fine-tuning Robustness
          </a>
          <div class="paper-authors">
            <span class="me">Vivek Ramanujan*</span>, Thao Nguyen*, Sewoong Oh, Ludwig Schmidt, Ali Farhadi
          </div>
          <div class="paper-venue">NeurIPS 2023 — Spotlight</div>
          <div class="paper-description">
            Data quantity is the primary factor influencing downstream robustness, while other factors have limited impact.
          </div>
        </article>

        <article class="paper">
          <a href="https://arxiv.org/abs/2306.10191" class="paper-title">
            Neural Priming for Sample-Efficient Adaptation
          </a>
          <div class="paper-authors">
            Matthew Wallingford*, <span class="me">Vivek Ramanujan*</span>, Alex Fang, Aditya Kusupati, Roozbeh Mottaghi, Aniruddha Kembhavi, Ludwig Schmidt, Ali Farhadi
          </div>
          <div class="paper-venue">NeurIPS 2023</div>
          <div class="paper-description">
            Enabling large pretrained models to adapt to distribution shifts with minimal labeled data.
          </div>
          <div class="paper-links">
            <a href="https://github.com/RAIVNLab/neural-priming">Code</a>
          </div>
        </article>

        <article class="paper">
          <a href="https://arxiv.org/abs/2304.14108" class="paper-title">
            DataComp: In Search of the Next Generation of Multimodal Datasets
          </a>
          <div class="paper-authors">
            Samir Yitzhak Gadre*, Gabriel Ilharco*, Alex Fang*, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, <span class="me">Vivek Ramanujan</span>, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, Ludwig Schmidt
          </div>
          <div class="paper-venue">NeurIPS 2023 — Datasets Track</div>
          <div class="paper-description">
            A benchmark for multimodal dataset creation with 12.8B image-text pairs. DataComp-1B achieves 79.2% zero-shot ImageNet accuracy.
          </div>
          <div class="paper-links">
            <a href="https://github.com/mlfoundations/datacomp">Code</a>
          </div>
        </article>

        <article class="paper">
          <a href="https://arxiv.org/abs/2301.04101" class="paper-title">
            Neural Radiance Field Codebooks
          </a>
          <div class="paper-authors">
            Matthew Wallingford, Aditya Kusupati, Alex Fang, <span class="me">Vivek Ramanujan</span>, Aniruddha Kembhavi, Roozbeh Mottaghi, Ali Farhadi
          </div>
          <div class="paper-venue">ICLR 2023</div>
          <div class="paper-description">
            Learning object-centric representations through novel view reconstruction using a dictionary of object codes.
          </div>
        </article>

        <article class="paper">
          <a href="https://arxiv.org/abs/2205.13147" class="paper-title">
            Matryoshka Representations for Adaptive Deployment
          </a>
          <div class="paper-authors">
            Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, <span class="me">Vivek Ramanujan</span>, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek Jain, Ali Farhadi
          </div>
          <div class="paper-venue">NeurIPS 2022</div>
          <div class="paper-description">
            Flexible representations that adapt to multiple downstream tasks with varying computational resources.
          </div>
          <div class="paper-links">
            <a href="https://github.com/RAIVNLab/MRL">Code</a>
          </div>
        </article>

        <article class="paper">
          <a href="https://arxiv.org/abs/2112.02805" class="paper-title">
            Forward Compatible Training for Representation Learning
          </a>
          <div class="paper-authors">
            <span class="me">Vivek Ramanujan</span>, Pavan Kumar Anasosalu Vasu, Ali Farhadi, Oncel Tuzel, Hadi Pouransari
          </div>
          <div class="paper-venue">CVPR 2022</div>
          <div class="paper-description">
            Preparing for future model versions by saving cheap auxiliary information about present training.
          </div>
          <div class="paper-links">
            <a href="https://github.com/apple/ml-fct">Code</a>
          </div>
        </article>

        <article class="paper">
          <a href="https://arxiv.org/abs/2010.09697" class="paper-title">
            Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent
          </a>
          <div class="paper-authors">
            Will Merrill, <span class="me">Vivek Ramanujan</span>, Yoav Goldberg, Roy Schwartz, Noah Smith
          </div>
          <div class="paper-venue">EMNLP 2022 — Oral</div>
          <div class="paper-description">
            As parameters grow in magnitude, networks approximate discretized networks with saturated activations—a new characterization of inductive bias in GD.
          </div>
        </article>

        <article class="paper">
          <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/c88d8d0a6097754525e02c2246d8d27f-Paper.pdf" class="paper-title">
            LLC: Accurate, Multi-Purpose Learnt Low-Dimensional Binary Codes
          </a>
          <div class="paper-authors">
            Aditya Kusupati, Matthew Wallingford, <span class="me">Vivek Ramanujan</span>, Raghav Somani, Jae Sung Park, Krishna Pillutla, Prateek Jain, Sham Kakade, Ali Farhadi
          </div>
          <div class="paper-venue">NeurIPS 2021</div>
          <div class="paper-description">
            Learning extremely low-dimensional binary codes (~20 bits for ImageNet-1K) while maintaining near-optimal accuracy.
          </div>
          <div class="paper-links">
            <a href="https://github.com/RAIVNLab/LLC">Code</a>
          </div>
        </article>

        <article class="paper">
          <a href="https://arxiv.org/abs/2006.14769" class="paper-title">
            Supermasks in Superposition
          </a>
          <div class="paper-authors">
            Mitchell Wortsman*, <span class="me">Vivek Ramanujan*</span>, Rosanne Liu, Aniruddha Kembhavi, Mohammad Rastegari, Jason Yosinski, Ali Farhadi
          </div>
          <div class="paper-venue">NeurIPS 2020</div>
          <div class="paper-description">
            Hidden networks for continual learning—learning thousands of tasks without catastrophic forgetting.
          </div>
          <div class="paper-links">
            <a href="https://github.com/RAIVNLab/supsup">Code</a>
            <a href="https://mitchellnw.github.io/blog/2020/supsup">Blog</a>
          </div>
        </article>

        <article class="paper">
          <a href="https://arxiv.org/abs/1911.13299" class="paper-title">
            What's Hidden in a Randomly Weighted Neural Network?
          </a>
          <div class="paper-authors">
            <span class="me">Vivek Ramanujan*</span>, Mitchell Wortsman*, Aniruddha Kembhavi, Ali Farhadi, Mohammad Rastegari
          </div>
          <div class="paper-venue">CVPR 2020</div>
          <div class="paper-description">
            Finding untrained subnetworks at initialization that match trained network performance.
          </div>
          <div class="paper-links">
            <a href="https://www.github.com/allenai/hidden-networks">Code</a>
          </div>
        </article>

        <article class="paper">
          <a href="https://arxiv.org/abs/2002.03231" class="paper-title">
            Soft Threshold Weight Reparameterization for Learnable Sparsity
          </a>
          <div class="paper-authors">
            Aditya Kusupati, <span class="me">Vivek Ramanujan</span>, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, Ali Farhadi
          </div>
          <div class="paper-venue">ICML 2020</div>
          <div class="paper-description">
            A pruning strategy based on soft threshold reparametrization, enabling very sparse but highly performant trained models.
          </div>
        </article>

        <article class="paper">
          <a href="https://arxiv.org/abs/1808.04325" class="paper-title">
            Improving Shape Deformation in Unsupervised Image-to-Image Translation
          </a>
          <div class="paper-authors">
            Aaron Gokaslan, <span class="me">Vivek Ramanujan</span>, Kwang-In Kim, Daniel Ritchie, James Tompkin
          </div>
          <div class="paper-venue">ECCV 2018</div>
          <div class="paper-description">
            Improving on CycleGAN by allowing better shape deformation between more disparate domains.
          </div>
        </article>
      </section>

      <section class="section">
        <h2>Service</h2>
        <div class="service-grid">
          <div class="service-item">
            <span class="role">Reviewer</span>
            <span class="event">CVPR 2025, ICLR 2024, NeurIPS 2024</span>
          </div>
          <div class="service-item">
            <span class="role">Teaching</span>
            <span class="event">CS146 Computer Vision, CS142 ML, CS141 Applied AI, CS2951K Deep Learning</span>
          </div>
          <div class="service-item">
            <span class="role">Fun</span>
            <span class="event"><a href="reaction_game.html">Reaction Game</a> · <a href="brown_noise.html">Brown Noise</a></span>
          </div>
        </div>
      </section>

      <footer class="footer">
        Vivek Ramanujan · Seattle, WA
      </footer>
    </div>

    <script>
      function toggleTheme() {
        const html = document.documentElement;
        const current = html.getAttribute('data-theme');
        const next = current === 'light' ? 'dark' : 'light';
        html.setAttribute('data-theme', next);
        document.getElementById('theme-icon').textContent = next === 'light' ? '◑' : '◐';
        localStorage.setItem('theme', next);
      }

      // Load saved theme
      const saved = localStorage.getItem('theme');
      if (saved) {
        document.documentElement.setAttribute('data-theme', saved);
        document.getElementById('theme-icon').textContent = saved === 'light' ? '◑' : '◐';
      } else if (window.matchMedia('(prefers-color-scheme: light)').matches) {
        document.documentElement.setAttribute('data-theme', 'light');
        document.getElementById('theme-icon').textContent = '◑';
      }
    </script>
  </body>
</html>
