<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>

  <title>Vivek Ramanujan</title>
  
  <meta name="author" content="Vivek Ramanujan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="favicon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/vivekr.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/vivekr.png" class="hoverZoomLink"></a>
            </td>
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Vivek Ramanujan</name>
              </p>
              <p>I am currently a PhD student at the <a href="https://cs.washington.edu">University of Washington</a> working with <a href="https://homes.cs.washington.edu/~ali/">Ali Farhadi</a> and <a href="https://people.csail.mit.edu/ludwigs/">Ludwig Schmidt</a> on problems related to robust machine learning. Previously, I was a predoctoral researcher on the PRIOR (vision) group at the <a href="https://prior.allenai.org">Allen Institute for Artificial Intelligence</a> (AI2), where I was advised by Mohammad Rastegari and Aniruddha Kembhavi.

              <p style="text-align:center">
                <a href="mailto:ramanv@cs.washington.edu">Email</a> &nbsp/&nbsp
                <a href="cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://www.github.com/vkramanuj">Github</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=yXFPyNMAAAAJ&hl=en">Google Scholar</a>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm broadly interested in computer vision, machine learning, and optimization. See my Google Scholar for a consistently up-to-date publication list. <!--- Representative papers are <span class="highlight">highlighted</span>. --->
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/teaser_fct.png" alt="clean-usnob" width="180">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2112.02805">
                <papertitle>Forward Compatible Training for Representation Learning</papertitle>
              </a>
              <br>
              <strong>Vivek Ramanujan</strong>, Pavan Kumar Anasosalu Vasu, Ali Farhadi, Oncel Tuzel, Hadi Pouransari
              <br>
              <em>In Proceedings at CVPR</em>, 2022
              <p>In real world visual retrieval systems, the embedding model is consistently updated. This requires embeddings for all images in the gallery to be recomputed for every new model, an expensive process known as backfilling. We present a method for forward compatible training (FCT) in which we prepare for the future version of a model by saving cheap auxiliary information about the present training task. We show empirically that this improves performance on model compatibility on common largescale datasets (ImageNet, Places-365, VGGFace2).</p>
              <p><a href="https://github.com/apple/ml-fct">Code</a> </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/teaser_param.png" alt="clean-usnob" width="180">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2010.09697">
                <papertitle>Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent</papertitle>
              </a>
              <br>
              Will Merrill, <strong>Vivek Ramanujan</strong>, Yoav Goldberg, Roy Schwartz, Noah Smith
              <br>
              <em>In Proceedings at EMNLP</em>, <em>(Oral)</em> 2022
              <p>The capacity of neural networks like the widely adopted transformer is known to be very high. Evidence is emerging that they learn successfully due to inductive bias in the training routine, typically a variant of gradient descent (GD). As the parameters grow in magnitude, we prove that the network approximates a discretized network with saturated activation functions. Such "saturated" networks are known to have a reduced capacity compared to the full network family that can be described in terms of formal languages and automata. Our results suggest saturation is a new characterization of an inductive bias implicit in GD of particular interest for NLP. We leverage the emergent discrete structure in a saturated transformer to analyze the role of different attention heads, finding that some focus locally on a small number of positions, while other heads compute global averages, allowing counting.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/teaser_supsup.png" alt="clean-usnob" width="180">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2006.14769">
                <papertitle>Supermasks in Superposition</papertitle>
              </a>
              <br>
              Mitchell Wortsman*, <strong>Vivek Ramanujan*</strong>, Rosanne Liu, Aniruddha Kembhavi, Mohammad Rastegari, Jason Yosinski, Ali Farhadi
              <br>
              <em>In Proceedings at NeurIPS</em>, 2020
              <p>We present an application of hidden networks for continual learning, capable of learning thousands of tasks without catastrophic forgetting. We solve tasks individually, each solution corresponding to a subnetwork of a randomly initialized neural network. Using a superposition of these subnetworks, we demonstrate that the viability of this model for task inference. Finally, we introduce a coherent hierarchy for continual learning problems. </p>
              <p><a href="https://github.com/RAIVNLab/supsup">Code</a> / <a href="https://mitchellnw.github.io/blog/2020/supsup">Blog</a></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/teaser_aditya.png" alt="clean-usnob" width="180">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2002.03231">
                <papertitle>Soft Threshold Weight Reparameterization for Learnable Sparsity</papertitle>
              </a>
              <br>
              Aditya Kusupati, <strong>Vivek Ramanujan</strong>, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, Ali Farhadi
              <br>
              To appear at the <em>International Conference on Machine Learning 2020</em>
              <p>We introduce a new strategy for pruning neural networks based off of the soft threshold reparametrization technique from signal processing. The layerwise sparsity budgets allow for very sparse but still highly performant trained models across a variety of architectures and tasks. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/teaser.png" alt="clean-usnob" width="180">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1911.13299">
                <papertitle>What's Hidden in a Randomly Weighted Neural Network?</papertitle>
              </a>
              <br>
              <strong>Vivek Ramanujan*</strong>, Mitchell Wortsman*, Aniruddha Kembhavi, Ali Farhadi, Mohammad Rastegari
              <br>
              <em>Computer Vision and Pattern Recognition 2020</em>
              <p>We demonstrate that you can find untrained subnetworks of common overparametrized convolutional neural networks <em>at initialization</em> that achieve performance similar to their densely trained counterparts.</p>
              <p><a href="https://www.github.com/allenai/hidden-networks">Code and Project Page</a></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ganimorph_s.jpg" alt="clean-usnob" width="180">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1808.04325">
                <papertitle>Improving Shape Deformation in Unsupervised Image-to-Image Translation</papertitle>
              </a>
              <br>
              Aaron Gokaslan, <strong>Vivek Ramanujan</strong>, Kwang-In Kim, Daniel Ritchie, James Tompkin
              <br>
              <em>European Conference for Computer Vision</em>, 2018
              <p>We improve on CycleGAN by allowing for better shape deformation between more disparate domains.</p>
            </td>
          </tr>

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/brown_logo.png" alt="Brown CS" width=160>
            </td>
            <td width="75%" valign="center">
              <a href="http://cs.brown.edu"> Teaching Assistant, Computer Vision CS146 Spring 2018</a>
              <br>
              <br>
              <a href="http://cs.brown.edu"> Teaching Assistant, Machine Learning CS142 Spring 2018</a>
              <br>
              <br>
              <a href="http://cs.brown.edu"> Teaching Assistant, Applied Artificial Intelligence CS141 Spring 2017</a>
              <br>
              <br>
              <a href="http://cs.brown.edu"> Teaching Assistant, Deep Learning CS2951K, Fall 2016</a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                This website's source is slightly modified from Jonathan Barron's <a href="https://github.com/jonbarron/jonbarron_website">website</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
